---
title: '2025年のAIトレンド展望：対話から行動へ（LAMs）—— AIがマウスクリックを覚えた時'
description: '大規模言語モデル（LLM）は「人間の言葉を理解する」問題を解決しました。2025年の焦点はLarge Action Model（LAM）であり、これによってAIは真にデジタル世界のUIに接続し、私たちに代わって煩雑な操作を完了します。'
pubDate: '2026-01-29'
heroImage: '../../../assets/blog-placeholder-1.jpg'
---

2023-2024年が **LLM（Large Language Model、大規模言語モデル）** の輝かしい時代だったとすれば、詩を書き、コードを書き、チューリングテストに合格することを学んだ時代でした。そして2025年は間違いなく **LAM（Large Action Model、大規模行動モデル）** の元年です。

AIはもはやチャットボックスでタイピングするだけの「キーボード戦士」に満足せず、「手」を生やし始め、私たちのマウスとキーボードを乗っ取り、アプリをクリックし、フォームに記入し、人間用に設計されたユーザーインターフェース（UI）を操作しようとしています。

## LAMとは？なぜ必要なのか？

### LLMの限界
来週東京行きの航空券を予約したいとします。
*   **LLMができること**：航空会社の比較リストを提供し、その季節の東京の天気を教え、休暇申請書まで書いてくれます。
*   **LLMができないこと**：実際にCtripやExpediaを開き、フライトを選択し、パスポート番号を入力し、「支払い」ボタンをクリックすること。

なぜなら、LLMが出力するのは **テキスト（Text）** であり、現実世界の多くのタスクには **行動（Action）** が必要だからです。

### LAMの定義
LAMは特別に訓練されたモデルで、自然言語だけでなく **GUI（グラフィカルユーザーインターフェース）** も理解します。スクリーンショット、DOMツリー、API呼び出しを理解することで、ユーザーの意図を一連の具体的な操作ステップに変換できます。

> **Rabbit R1の啓示**：2024年のRabbit R1のハードウェアリリースは議論を呼びましたが、提案されたLAMコンセプトの方向性は正しいものでした。Spotify、Uberなどのソフトウェアを、それらがAPIを公開しているかどうかに依存せず、人間のように操作する方法をAIに学習させようとしました。

## 技術原理：AIはどのように「見て」「クリック」するのか？

LAMを実現するには主に2つの技術ルートがあり、2026年にはこれら2つのルートが融合しつつあります。

### 1. APIベースの関数呼び出し（Function Calling）
これは現在の過渡的なソリューションです。OpenAIのAssistant APIのように、一連のツール（`get_weather`、`book_flight`など）を定義すると、モデルはJSON形式の呼び出し命令を出力します。
*   **利点**：正確で安定している。
*   **欠点**：サービスプロバイダーがAPIを公開しているかどうかに制限される。あるアプリにAPIがない場合、AIは手も足も出ません。

### 2. ビジュアルベースのUI操作（Visual UI Action）
これはLAMの究極の形態です。AIは人間のように画面を「見る」（Computer Vision技術を通じて）。
1.  **知覚（See）**：モデルは現在のスクリーンショットを分析し、どこがボタンで、どこが入力フィールドで、どこがドロップダウンメニューかを識別します。
2.  **計画（Plan）**：ユーザーの指示「ラテを注文して」に基づいて、パスを計画 -> デリバリーアプリを開く -> 検索をクリック -> ラテを入力 -> 選択 -> 決済。
3.  **実行（Act）**：マウスの動きとキーボード入力をシミュレート。

**マルチモーダル（Multi-Modal）** 技術の発展により、このルートが現実のものとなりました。GPT-4VとGemini 1.5 Proは驚異的な画面理解能力を示しています。

## 2025-2026年のアプリケーションシーンの爆発

### 1. ソフトウェア操作の自動化（RPA 2.0）
従来のRPA（ロボティック・プロセス・オートメーション）では、プログラマーが硬直的にスクリプトを記録する必要があり、UIがわずかに変更されただけで（ボタンの位置が5ピクセル移動したなど）、スクリプトが壊れてしまいました。
LAM駆動のRPAは**セマンティックレベル**です。「送信ボタン」を探すのであって、「座標（800, 600）」ではありません。インターフェースが大幅に改訂されても、ロジックが変わらない限り、AIはボタンを見つけることができます。これは企業内部の経費精算、承認、データ入力プロセスを完全に再構築します。

### 2. スーパーパーソナルアシスタント
スマートフォンのOSレベルでシステムレベルのLAMが組み込まれます。
*   Siri / Android Assistantはもはや役立たずではありません。「さっき送ってくれた写真を編集して、SNSに投稿して、『残業中でも人生を愛する』ってキャプションつけて」と言えます。
*   AIは自動的にアルバムを開く -> 編集機能を呼び出す -> WeChatを開く -> モーメントを編集 -> 送信。全プロセスがスムーズです。

## 直面する課題：セキュリティと信頼

AIに「行動権」を与えることは、「発言権」を与えることより1万倍危険です。

*   **誤操作のリスク**：AIが誤解して、「100元送金」を「10000元送金」と理解したり、メールを間違った人に送ったりすると、結果は想像を絶します。
*   **人間参加型（Human-in-the-loop）**：2026年の設計規範では、すべての機密操作（支払い、削除、送信）に「人間の確認」ステップが必要です。AIは記入のみを担当し、「確定」をクリックする指は人間のものでなければなりません。

## 結語

LLMからLAMへ、これはAIが**情報提供者（Info-Provider）** から **サービス提供者（Service-Provider）** への飛躍です。私たちは「口で言うだけで手を動かさない」時代を迎えようとしています。この時代において、最も貴重なスキルは複雑なソフトウェアを操作することではなく、自分の意図を明確に表現することです。
